# edgar_downloader.py
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¢å¼ºç¨³å®šæ€§ã€è¾¹ç¼˜æƒ…å†µå¤„ç†å’Œæ—¥å¿—å¯è§‚æµ‹æ€§

import os
import re
import requests
import time
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from urllib.parse import urlparse

class EdgarDownloader:
    def __init__(self, logger, max_retries=3):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        :param logger: æ—¥å¿—è®°å½•å™¨
        :param max_retries: å•æ–‡ä»¶ä¸‹è½½æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.logger = logger
        self.max_retries = max_retries
        self.headers = {
            "User-Agent": "QuantMLLabBot/1.0 (Contact: mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml"
        }
        # SECæ¨èçš„æœ€å°è¯·æ±‚é—´éš”(ç§’)
        self.request_interval = 0.5  

    def download_latest_form4(self, days_back=3):
        """
        ä¸‹è½½æœ€è¿‘Nå¤©çš„Form 4ç”³æŠ¥æ–‡ä»¶
        :param days_back: å›æº¯å¤©æ•°
        :return: æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        save_dir = os.path.abspath("data/form4")
        os.makedirs(save_dir, exist_ok=True)
        
        self.logger.info(f"ğŸš€ å¼€å§‹è·å–æœ€è¿‘ {days_back} å¤©çš„Form 4ç”³æŠ¥æ–‡ä»¶")
        downloaded_files = []

        try:
            # 1. è·å–Atom Feedåˆ—è¡¨
            feed_url = self._build_feed_url(days_back)
            entries = self._fetch_atom_feed(feed_url)
            if not entries:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•ç”³æŠ¥æ¡ç›®")
                return []

            # 2. å¤„ç†æ¯ä¸ªç”³æŠ¥æ¡ç›®
            for idx, entry in enumerate(entries, 1):
                try:
                    filepath = self._process_entry(entry, save_dir)
                    if filepath:
                        downloaded_files.append(filepath)
                        self.logger.debug(f"è¿›åº¦: {idx}/{len(entries)}")
                except Exception as e:
                    self.logger.error(f"å¤„ç†æ¡ç›®å¤±è´¥: {str(e)}", exc_info=True)
                finally:
                    time.sleep(self.request_interval)

        except Exception as e:
            self.logger.critical(f"ä¸»æµç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
        
        self.logger.info(f"ğŸ‰ å®Œæˆ! æˆåŠŸä¸‹è½½ {len(downloaded_files)}/{len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back):
        """æ„å»ºAtom Feed URLï¼ˆè€ƒè™‘æ—¶åŒºï¼‰"""
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=days_back)
        return (
            "https://www.sec.gov/cgi-bin/browse-edgar"
            f"?action=getcurrent&type=4"
            f"&datea={start_date.strftime('%Y%m%d')}"
            "&output=atom"
        )

    def _fetch_atom_feed(self, url):
        """è·å–å¹¶è§£æAtom Feed"""
        try:
            resp = requests.get(url, headers=self.headers, timeout=15)
            resp.raise_for_status()
            
            # éªŒè¯å†…å®¹ç±»å‹
            if "application/atom+xml" not in resp.headers.get("Content-Type", ""):
                raise ValueError("æ— æ•ˆçš„Atom Feedå†…å®¹ç±»å‹")
                
            soup = BeautifulSoup(resp.text, "xml")
            return soup.find_all("entry")
        except Exception as e:
            self.logger.error(f"è·å–Atom Feedå¤±è´¥: {str(e)}")
            raise

    def _process_entry(self, entry, save_dir):
        """å¤„ç†å•ä¸ªAtomæ¡ç›®"""
        if not entry.link:
            raise ValueError("æ¡ç›®ç¼ºå°‘linkæ ‡ç­¾")
            
        # 1. æå–å…ƒæ•°æ®
        filing_url = entry.link["href"]
        company_name = entry.find("title").text.split(" - ")[0]
        updated = entry.find("updated").text if entry.find("updated") else "N/A"
        
        # 2. æ„å»ºXMLæ–‡ä»¶URLï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
        xml_url = self._normalize_xml_url(filing_url)
        if not xml_url:
            raise ValueError("æ— æ³•è§„èŒƒåŒ–XML URL")
            
        # 3. ä¸‹è½½æ–‡ä»¶
        cik, accession = self._extract_identifiers(xml_url)
        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)
        
        self.logger.info(
            f"ä¸‹è½½ {company_name} çš„ç”³æŠ¥ (CIK:{cik}, æ›´æ–°:{updated})"
        )
        
        content = self._download_with_retry(xml_url)
        if content:
            with open(filepath, "wb") as f:
                f.write(content)
            return filepath
        return None

    def _normalize_xml_url(self, filing_url):
        """è§„èŒƒåŒ–XMLæ–‡ä»¶URLï¼ˆå¤„ç†å†å²æ ¼å¼å˜åŒ–ï¼‰"""
        base_url = filing_url.replace("-index.htm", "").replace("-index.html", "")
        
        # å°è¯•å¸¸è§æ ¼å¼
        for ext in ["/primary_doc.xml", ".xml", "/index.xml"]:
            test_url = base_url + ext
            if self._validate_url(test_url):
                return test_url
        return None

    def _validate_url(self, url):
        """éªŒè¯URLæ ¼å¼æœ‰æ•ˆæ€§"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc, result.path])
        except:
            return False

    def _extract_identifiers(self, url):
        """ä»URLæå–CIKå’ŒAccession Number"""
        match = re.search(r"data/(\d{10})/([^/]+?)(/|\.xml|$)", url)
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        cik, accession = match.groups()[:2]
        return cik.zfill(10), accession.replace("-", "")

    def _download_with_retry(self, url):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ–‡ä»¶ä¸‹è½½"""
        for attempt in range(self.max_retries):
            try:
                resp = requests.get(
                    url,
                    headers=self.headers,
                    timeout=15,
                    stream=True  # æµå¼ä¸‹è½½é¿å…å¤§æ–‡ä»¶å†…å­˜é—®é¢˜
                )
                resp.raise_for_status()
                
                # éªŒè¯å†…å®¹
                if resp.status_code == 200 and "xml" in resp.headers.get("Content-Type", "").lower():
                    return resp.content
                    
            except requests.RequestException as e:
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    self.logger.warning(
                        f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}), "
                        f"{wait_time}ç§’åé‡è¯•: {url}"
                    )
                    time.sleep(wait_time)
                else:
                    self.logger.error(
                        f"ä¸‹è½½å¤±è´¥ (æœ€ç»ˆå°è¯•): {url}, é”™è¯¯: {str(e)}"
                    )
        return None
