import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import requests
from bs4 import BeautifulSoup


class EdgarDownloader:
    """
    ç®€åŒ–ç‰ˆ EdgarDownloaderï¼šç”¨äºä¸‹è½½ SEC Form 4 XML æ–‡ä»¶ã€‚
    
    âœ… ä»…ä¾èµ– <category term="4"> åˆ¤æ–­æ˜¯å¦ä¸º Form 4
    âœ… ä¸å†éªŒè¯ XML å†…å®¹ç»“æ„
    âœ… æ”¯æŒ index é¡µé¢è§£æ å’Œ fallback è·¯å¾„æ‹¼æ¥ä¸‹è½½
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ– downloader å¯¹è±¡

        Args:
            logger: æ—¥å¿—è®°å½•å™¨å¯¹è±¡
            max_retries: æ¯ä¸ªé“¾æ¥æœ€å¤šé‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡ HTTP è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            request_interval: æ¯ä¸ªæ¡ç›®ä¸‹è½½ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
            base_url: SEC ä¸»ç«™ URL
            cache_dir: ä¸‹è½½ç¼“å­˜ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval

        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 7, save_dir: str = "data/form4") -> List[str]:
        """
        ä¸»å‡½æ•°ï¼šä¸‹è½½æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š
        
        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: XML æ–‡ä»¶ä¿å­˜ç›®å½•

        Returns:
            æ‰€æœ‰æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}")
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        æ„é€  Atom Feed è¯·æ±‚é“¾æ¥ï¼Œç”¨äºè·å–æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š

        Args:
            days_back: å›æº¯å¤©æ•°

        Returns:
            å®Œæ•´ URL å­—ç¬¦ä¸²
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",  # Form 4 ç±»å‹
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """
        è¯·æ±‚ Atom Feed å¹¶ç­›é€‰å‡º <category term="4"> çš„ Form 4 æŠ¥å‘Š

        Args:
            url: Feed é¡µé¢ URL

        Returns:
            æ‰€æœ‰ Form 4 ç±»å‹çš„ entry åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")
        entries = soup.find_all("entry")

        # âœ… åªä¿ç•™ <category term="4"> çš„æ¡ç›®
        form4_entries = [e for e in entries if e.find("category", {"term": "4"})]
        self.logger.info(f"ğŸ¯ å…± {len(entries)} ä¸ªæ¡ç›®ï¼Œç­›é€‰å‡º {len(form4_entries)} ä¸ª Form 4")
        return form4_entries

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """
        å¤„ç†å•ä¸ª Form 4 entryï¼Œä¸‹è½½å¹¶ä¿å­˜ XML æ–‡ä»¶

        Args:
            entry: å•ä¸ª <entry> èŠ‚ç‚¹
            save_dir: æœ¬åœ°ä¿å­˜ç›®å½•

        Returns:
            æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ– None
        """
        filing_url = entry.link["href"]
        cik, accession = self._extract_identifiers(filing_url)
        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        if os.path.exists(filepath):
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥: {filing_url}")
            return None

        with open(filepath, "wb") as f:
            f.write(content)
        return filepath

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """
        å°è¯•å¤šç§ URL ä¸‹è½½ XML æ–‡ä»¶ï¼ˆindex é¡µé¢ + fallback è·¯å¾„ï¼‰

        Returns:
            ä¸‹è½½åˆ°çš„ XML å†…å®¹ï¼Œæˆ– None
        """
        xml_urls = self._get_xml_urls_from_index(filing_url)
        if not xml_urls:
            xml_urls = self._generate_candidate_urls(filing_url, cik, accession)

        for url in xml_urls:
            content = self._try_download(url)
            if content:
                return content
        return None

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        å°è¯•è§£æ index é¡µé¢ï¼Œæå– XML æ–‡ä»¶çœŸå®è·¯å¾„

        Returns:
            æ‰€æœ‰æ‰¾åˆ°çš„ XML ä¸‹è½½é“¾æ¥
        """
        try:
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                return []
            soup = BeautifulSoup(response.content, "html.parser")
            xml_urls = []

            for link in soup.find_all("a", href=True):
                href = link["href"]
                if href.endswith(".xml"):
                    if href.startswith("/"):
                        full_url = self.base_url + href
                    else:
                        full_url = urljoin(filing_url, href)
                    xml_urls.append(full_url)

            return list(dict.fromkeys(xml_urls))
        except Exception:
            return []

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        ä» URL ä¸­æå– CIK å’Œ accession number
        
        Raises:
            ValueError: å¦‚æœæ ¼å¼ä¸åŒ¹é…
        """
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url)
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        cik = match.group(1).zfill(10)
        accession = re.sub(r'[^\d\-]', '', match.group(2))
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        åœ¨æ— æ³•è§£æ index é¡µæ—¶ï¼Œæ‹¼å‡ºæ‰€æœ‰å¸¸è§ Form 4 XML æ–‡ä»¶åä½œä¸ºå€™é€‰é“¾æ¥
        """
        clean_accession = ''.join(c for c in accession if c.isdigit())
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}"
        return list(dict.fromkeys([
            filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"),
            f"{base_path}/primary_doc.xml",
            f"{base_path}/form4.xml",
            f"{base_path}/doc4.xml",
            f"{base_path}/{accession}.xml",
            f"{base_path}/wf-form4_{clean_accession}.xml",
            f"{base_path}/xslForm4_{clean_accession}.xml"
        ]))

    def _try_download(self, url: str) -> Optional[bytes]:
        """
        å•é“¾æ¥ä¸‹è½½ï¼Œæ”¯æŒç¼“å­˜å’Œæœ€å¤š N æ¬¡é‡è¯•

        Returns:
            æˆåŠŸçš„äºŒè¿›åˆ¶å†…å®¹æˆ– None
        """
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_path = os.path.join(self.cache_dir, cache_key)

        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                return f.read()

        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout)
                if response.status_code == 200:
                    content = response.content
                    with open(cache_path, "wb") as f:
                        f.write(content)
                    return content
            except Exception:
                time.sleep(min(2 ** attempt, 10))  # æŒ‡æ•°é€€é¿
        return None
