import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET # Import for XML validation


class EdgarDownloader:
    """
    ç®€åŒ–ç‰ˆ EdgarDownloaderï¼šç”¨äºä¸‹è½½ SEC Form 4 XML æ–‡ä»¶ã€‚
    
    âœ… ä»…ä¾èµ– <category term="4"> åˆ¤æ–­æ˜¯å¦ä¸º Form 4
    âœ… ä¸å†éªŒè¯ XML å†…å®¹ç»“æ„
    âœ… æ”¯æŒ index é¡µé¢è§£æ å’Œ fallback è·¯å¾„æ‹¼æ¥ä¸‹è½½
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ– downloader å¯¹è±¡

        Args:
            logger: æ—¥å¿—è®°å½•å™¨å¯¹è±¡
            max_retries: æ¯ä¸ªé“¾æ¥æœ€å¤šé‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡ HTTP è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            request_interval: æ¯ä¸ªæ¡ç›®ä¸‹è½½ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
            base_url: SEC ä¸»ç«™ URL
            cache_dir: ä¸‹è½½ç¼“å­˜ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval

        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 7, save_dir: str = "data/form4") -> List[str]:
        """
        ä¸»å‡½æ•°ï¼šä¸‹è½½æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š
        
        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: XML æ–‡ä»¶ä¿å­˜ç›®å½•

        Returns:
            æ‰€æœ‰æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        # Log the total number of entries to process
        self.logger.info(f"ğŸ“¥ å³å°†ä¸‹è½½ Form 4 æ–‡ä»¶æ•°: {len(entries)}")

        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæˆ–æ— æ³•ä¸‹è½½çš„æ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}", exc_info=True) # Print detailed stack trace
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        Constructs the Atom Feed request URL to get Form 4 reports from the last N days.

        Args:
            days_back: Number of days to look back.

        Returns:
            Full URL string.
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",  # Form 4 type
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """
        Requests the Atom Feed and filters out Form 4 reports with <category term="4">.

        Args:
            url: Feed page URL.

        Returns:
            List of all Form 4 type entries.
        """
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status() # Check for HTTP errors
            soup = BeautifulSoup(response.content, "xml")
            entries = soup.find_all("entry")

            # âœ… Only keep entries with <category term="4">
            form4_entries = [e for e in entries if e.find("category", {"term": "4"})]
            self.logger.info(f"ğŸ¯ å…± {len(entries)} ä¸ªæ¡ç›®ï¼Œç­›é€‰å‡º {len(form4_entries)} ä¸ª Form 4")
            return form4_entries
        except requests.exceptions.RequestException as e:
            self.logger.error(f"åŠ è½½ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []
        except Exception as e:
            self.logger.error(f"è§£æ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """
        Processes a single Form 4 entry, downloads and saves the XML file.

        Args:
            entry: Single <entry> node.
            save_dir: Local save directory.

        Returns:
            Path to the successfully saved file, or None.
        """
        filing_url = entry.link["href"]
        try:
            cik, accession = self._extract_identifiers(filing_url)
        except ValueError as e:
            self.logger.warning(f"ğŸš« æ— æ³•ä» filing URL æå–æ ‡è¯†ç¬¦ï¼Œè·³è¿‡: {filing_url}ï¼Œé”™è¯¯: {e}")
            return None

        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        if os.path.exists(filepath):
            # This can be extended to re-download if the cached file is found invalid
            # For now, it skips if the file exists, as the primary validation is during download
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥ï¼Œæ— æ³•è·å– XML å†…å®¹: {filing_url}")
            return None

        try:
            with open(filepath, "wb") as f:
                f.write(content)
            return filepath
        except IOError as e:
            self.logger.error(f"ğŸš« å†™å…¥æ–‡ä»¶å¤±è´¥: {filepath}ï¼Œé”™è¯¯: {e}")
            return None

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """
        Attempts to download the XML file from multiple URLs (index page + fallback paths).

        Returns:
            Downloaded XML content, or None.
        """
        xml_urls = self._get_xml_urls_from_index(filing_url)
        if not xml_urls:
            # If no XML URLs were extracted from the index page, generate candidate URLs
            xml_urls = self._generate_candidate_urls(filing_url, cik, accession)

        # Ensure there is at least one URL to try, even fallback might be empty
        if not xml_urls:
            self.logger.warning(f"æ— æ³•ä¸º {filing_url} ç”Ÿæˆä»»ä½•å€™é€‰ XML ä¸‹è½½é“¾æ¥ã€‚")
            return None

        for url in xml_urls:
            content = self._try_download(url)
            if content:
                return content
        return None

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        å°è¯•è§£æ index é¡µé¢ï¼Œæå– XML æ–‡ä»¶çœŸå®è·¯å¾„ã€‚
        ä¿®æ”¹ï¼šæ›´æ™ºèƒ½åœ°å¤„ç†ä»ç´¢å¼•é¡µè·å–çš„XML URLï¼Œç¡®ä¿å®ƒç›´æ¥ä½äºå½’æ¡£è·¯å¾„ä¸‹ï¼Œ
        å»é™¤å¯èƒ½å­˜åœ¨çš„å­ç›®å½•è·¯å¾„ï¼Œå¦‚ "xslF345X05/"ã€‚

        Returns:
            æ‰€æœ‰æ‰¾åˆ°çš„ XML ä¸‹è½½é“¾æ¥
        """
        try:
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                self.logger.debug(f"è®¿é—®ç´¢å¼•é¡µå¤±è´¥: {filing_url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, "html.parser") 
            xml_urls = []

            # Extract CIK and Accession from the filing_url to construct the expected base path
            # Example filing_url: https://www.sec.gov/Archives/edgar/data/17313/000001731325000061/0000017313-25-000061-index.htm
            filing_url_parts = filing_url.split('/')
            
            try:
                # Find the "data" segment and then CIK and Accession
                data_idx = filing_url_parts.index('data')
                cik_from_url = filing_url_parts[data_idx + 1]
                accession_from_url = filing_url_parts[data_idx + 2]
                # Construct the direct base path for the XML files
                # This should be the path where primary XMLs usually reside
                expected_base_archive_path = urljoin(self.base_url, 
                                                     f"/Archives/edgar/data/{cik_from_url}/{accession_from_url}/")
            except (ValueError, IndexError):
                self.logger.warning(f"æ— æ³•ä» filing URL {filing_url} æå– CIK å’Œ Accessionï¼Œæ— æ³•ç¡®å®šé¢„æœŸXMLåŸºè·¯å¾„ã€‚å°†ä½¿ç”¨ filing_url çš„åŸºè·¯å¾„ä½œä¸ºå›é€€ã€‚")
                # Fallback: if CIK/Accession cannot be extracted from the URL structure, use the base of the filing URL itself.
                # This might still lead to subdirectories if the filing_url itself contains them.
                parsed_filing_url = urlparse(filing_url)
                expected_base_archive_path = f"{parsed_filing_url.scheme}://{parsed_filing_url.netloc}{os.path.dirname(parsed_filing_url.path)}/"


            for link in soup.find_all("a", href=True):
                href = link["href"]
                # Look for links ending in .xml and containing "form" (heuristic to exclude stylesheets like .xsl)
                # Convert href to lowercase for robust matching
                if href.lower().endswith(".xml") and "form" in href.lower():
                    # Get the actual filename from the href (e.g., wk-form4_1750708963.xml from xslF345X05/wk-form4_1750708963.xml)
                    xml_filename = os.path.basename(href) 

                    # Construct the full URL directly under the expected base archive path.
                    # This explicitly removes any intermediate directories from the href.
                    full_url = urljoin(expected_base_archive_path, xml_filename)
                    xml_urls.append(full_url)

            # Use dict.fromkeys to remove duplicates and convert back to list
            return list(dict.fromkeys(xml_urls))
        except requests.exceptions.RequestException as e:
            self.logger.debug(f"å°è¯•è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.debug(f"è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        Extracts CIK and accession number from the URL.
        
        Raises:
            ValueError: If the format does not match.
        """
        # Modified regex to more precisely match the accession number, especially before the final '-index.htm'
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url) 
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        cik = match.group(1).zfill(10) # Ensure CIK is 10 digits, padded with leading zeros
        # Accession number is usually 18 digits. Remove non-digit and non-hyphen characters,
        # but preserve hyphens for the original structure to match file naming conventions.
        # SEC's accession number format is YYYYMMDD-XXXXXX-XXXXX, here we retain hyphens.
        accession_raw = match.group(2)
        accession = re.sub(r'[^0-9\-]', '', accession_raw) 
        # Example: 0001127602-25-017854
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        When the index page cannot be parsed, constructs all common Form 4 XML filenames as candidate links.
        """
        # Clean the accession number, keeping only digits for filename construction
        clean_accession = ''.join(c for c in accession if c.isdigit())
        # Construct the base path, converting CIK to int to remove leading zeros, then back to string
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}"
        
        candidate_urls = [
            # 1. Try directly replacing -index.htm/.html with .xml in the index page URL
            filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"),
            # 2. Common generic XML filenames
            f"{base_path}/primary_doc.xml",
            f"{base_path}/form4.xml",
            f"{base_path}/doc4.xml",
            # 3. XML with accession number as filename
            f"{base_path}/{accession}.xml", # Original accession with hyphens
            f"{base_path}/{clean_accession}.xml", # Purely numeric accession
            # 4. Common SEC-generated filename patterns
            f"{base_path}/wf-form4_{clean_accession}.xml",
            f"{base_path}/xslForm4_{clean_accession}.xml",
            f"{base_path}/nc-form4_{clean_accession}.xml", # Another common pattern
            f"{base_path}/e{clean_accession}.xml" # Pattern starting with 'e'
        ]
        # Use dict.fromkeys to remove duplicates and preserve order
        return list(dict.fromkeys(candidate_urls))

    def _try_download(self, url: str) -> Optional[bytes]:
        """
        Downloads a single link, supports caching and up to N retries.
        Adds XML content validity check.

        Returns:
            Successful binary content or None.
        """
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_path = os.path.join(self.cache_dir, cache_key)

        # Check cache, if it exists and is valid, return directly
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                content = f.read()
                try:
                    ET.fromstring(content) # Attempt to parse, ensure cached file is valid XML
                    self.logger.debug(f"ä»ç¼“å­˜åŠ è½½å¹¶éªŒè¯æˆåŠŸ: {os.path.basename(url)}")
                    return content
                except ET.ParseError:
                    self.logger.warning(f"ç¼“å­˜æ–‡ä»¶ {os.path.basename(url)} XML æ ¼å¼æ— æ•ˆï¼Œå°è¯•é‡æ–°ä¸‹è½½ã€‚")
                    os.remove(cache_path) # Delete invalid cached file

        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status() # Check HTTP status code (4xx, 5xx)

                content = response.content
                # !!! Key modification: Validate XML content before saving !!!
                try:
                    ET.fromstring(content) # Attempt to parse downloaded content, raises ParseError if not valid XML
                except ET.ParseError:
                    self.logger.warning(f"ä¸‹è½½æ–‡ä»¶ {url} XML æ ¼å¼æ— æ•ˆï¼ˆéè‰¯å¥½æ ¼å¼XMLï¼‰ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                    time.sleep(min(2 ** (attempt + 1), 10)) # Exponential backoff, starting from 1 second
                    continue # Skip this loop, go to next retry

                # If XML content is valid, save to cache
                with open(cache_path, "wb") as f:
                    f.write(content)
                return content
            except requests.exceptions.RequestException as e: # Catch requests library errors (network issues, timeouts, HTTP errors, etc.)
                self.logger.warning(f"ä¸‹è½½ {url} å¤±è´¥: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                time.sleep(min(2 ** (attempt + 1), 10)) # Exponential backoff, starting from 1 second
            except Exception as e: # Catch other unknown errors
                self.logger.error(f"ä¸‹è½½ {url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})", exc_info=True)
                time.sleep(min(2 ** (attempt + 1), 10))
        
        self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•æˆåŠŸä¸‹è½½å¹¶éªŒè¯ {url}")
        return None

