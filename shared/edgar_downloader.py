# âœ… ä¿®å¤åçš„ `edgar_downloader.py` - è§£å†³ XML æ–‡ä»¶å®šä½é—®é¢˜
# ä¸»è¦ä¿®å¤ï¼š
# 1. æ”¹è¿› XML æ–‡ä»¶ URL æ„é€ ç­–ç•¥
# 2. å…ˆè§£æ index é¡µé¢è·å–å®é™…çš„ XML æ–‡ä»¶å
# 3. å¢åŠ æ›´å¤šå€™é€‰ URL æ¨¡å¼
# 4. æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
# æœ€åä¿®æ”¹æ—¶é—´: 6/27/25

import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import requests
from bs4 import BeautifulSoup

class EdgarDownloader:
    """
    ç”¨äºä» SEC EDGAR Atom Feed ä¸‹è½½æœ€è¿‘ Form 4 æŠ¥å‘Šçš„æ ¸å¿ƒç±»ã€‚
    æ”¯æŒè‡ªåŠ¨ URL ä¿®å¤ã€å†…å®¹éªŒè¯ã€é‡è¯•æœºåˆ¶ä¸ç¼“å­˜ç­–ç•¥ã€‚
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡è¯·æ±‚è¶…æ—¶
            request_interval: è¯·æ±‚ä¹‹é—´çš„é—´éš”ç§’æ•°
            base_url: SEC åŸºç¡€åœ°å€
            cache_dir: ç”¨äºå­˜å‚¨ç¼“å­˜çš„ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval
        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 7, save_dir: str = "data/form4", max_workers: int = 1) -> List[str]:
        """
        ä¸‹è½½æœ€è¿‘ N å¤©çš„ Form 4 XML æ–‡ä»¶

        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: æœ¬åœ°ä¿å­˜ç›®å½•
            max_workers: å¹¶å‘æ•°ï¼ˆæš‚æœªä½¿ç”¨ï¼‰

        Returns:
            ä¸‹è½½æˆåŠŸçš„ XML æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}")
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        æ„é€  SEC Atom Feed URL

        Args:
            days_back: å›æº¯å¤©æ•°

        Returns:
            å®Œæ•´ URL
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """è¯·æ±‚å¹¶è§£æ Atom Feed"""
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")
        return soup.find_all("entry")

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """å¤„ç†å•ä¸ª Atom é¡¹ï¼Œå°è¯•ä¸‹è½½å¹¶ä¿å­˜å¯¹åº” Form 4 XML"""
        filing_url = entry.link["href"]
        cik, accession = self._extract_identifiers(filing_url)
        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(filepath):
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥: {filing_url}")
            return None

        with open(filepath, "wb") as f:
            f.write(content)
        return filepath

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """å°è¯•å¤šç§æ–¹å¼ä¸‹è½½ Form 4 XML å†…å®¹"""
        # é¦–å…ˆå°è¯•ä» index é¡µé¢è§£æå®é™…çš„ XML æ–‡ä»¶å
        xml_urls = self._get_xml_urls_from_index(filing_url)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„å€™é€‰ URL æ–¹æ³•
        if not xml_urls:
            xml_urls = self._generate_candidate_urls(filing_url, cik, accession)
        
        for url in xml_urls:
            content = self._try_download(url)
            if content:
                self.logger.debug(f"âœ… æˆåŠŸä¸‹è½½: {url}")
                return content
        
        return None

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        ä» filing index é¡µé¢è§£æå‡ºå®é™…çš„ XML æ–‡ä»¶é“¾æ¥
        """
        try:
            self.logger.debug(f"è§£æ index é¡µé¢: {filing_url}")
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.content, "html.parser")
            xml_urls = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ XML æ–‡ä»¶é“¾æ¥
            for link in soup.find_all("a", href=True):
                href = link["href"]
                # æŸ¥æ‰¾ Form 4 ç›¸å…³çš„ XML æ–‡ä»¶
                if any(pattern in href.lower() for pattern in [
                    "wf-form4", "xslform4", "form4.xml", ".xml"
                ]) and href.endswith(".xml"):
                    if href.startswith("/"):
                        full_url = self.base_url + href
                    else:
                        full_url = urljoin(filing_url, href)
                    xml_urls.append(full_url)
            
            # å»é‡å¹¶ä¼˜å…ˆé€‰æ‹© Form 4 ç›¸å…³çš„æ–‡ä»¶
            xml_urls = list(set(xml_urls))
            xml_urls.sort(key=lambda x: (
                "wf-form4" not in x.lower(),
                "xslform4" not in x.lower(),
                "form4" not in x.lower()
            ))
            
            self.logger.debug(f"ä» index é¡µé¢æ‰¾åˆ° {len(xml_urls)} ä¸ª XML å€™é€‰")
            return xml_urls
            
        except Exception as e:
            self.logger.debug(f"è§£æ index é¡µé¢å¤±è´¥: {e}")
            return []

    def _try_download(self, url: str) -> Optional[bytes]:
        """æ‰§è¡Œå•æ¬¡ä¸‹è½½å°è¯•ï¼ŒåŒ…å«ç¼“å­˜æœºåˆ¶"""
        cache_key = self._get_cache_key(url)
        cache_path = os.path.join(self.cache_dir, cache_key)

        # ä¼˜å…ˆè¿”å›ç¼“å­˜
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                content = f.read()
                if self._is_valid_form4_xml(content):
                    return content

        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"å°è¯•ä¸‹è½½ [{attempt+1}/{self.max_retries}]: {url}")
                response = self.session.get(url, timeout=self.timeout)
                
                if response.status_code == 200:
                    content = response.content
                    if self._is_valid_form4_xml(content):
                        # ç¼“å­˜æœ‰æ•ˆå†…å®¹
                        with open(cache_path, "wb") as f:
                            f.write(content)
                        return content
                    else:
                        self.logger.debug(f"å†…å®¹éªŒè¯å¤±è´¥: {url}")
                else:
                    self.logger.debug(f"HTTP {response.status_code}: {url}")
                    
            except Exception as e:
                wait = min(2 ** attempt, 10)  # æœ€å¤§ç­‰å¾… 10 ç§’
                self.logger.debug(f"ä¸‹è½½å¼‚å¸¸ [{attempt+1}/{self.max_retries}] {url}ï¼š{e}")
                if attempt < self.max_retries - 1:
                    time.sleep(wait)
        
        return None

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        ä» URL ä¸­æå– CIK å’Œ Accession Number

        Raises:
            ValueError: æ— æ³•è¯†åˆ«çš„ URL æ ¼å¼
        """
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url)
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        
        cik = match.group(1).zfill(10)
        accession_raw = match.group(2)
        # æ¸…ç† accession numberï¼Œä¿ç•™æ•°å­—å’Œè¿å­—ç¬¦
        accession = re.sub(r'[^\d\-]', '', accession_raw)
        
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        æ„é€ å¤šä¸ªå€™é€‰ XML ä¸‹è½½åœ°å€ä»¥æå‡å®¹é”™æ€§
        """
        clean_accession = ''.join(c for c in accession if c.isdigit())
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}"
        
        candidates = []
        
        # æ–¹æ³•1: ç®€å•æ›¿æ¢ index åç¼€
        candidates.append(filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"))
        
        # æ–¹æ³•2: å¸¸è§çš„ Form 4 XML æ–‡ä»¶åæ¨¡å¼
        candidates.extend([
            f"{base_path}/wf-form4_{clean_accession}.xml",
            f"{base_path}/xslForm4_{clean_accession}.xml",
            f"{base_path}/form4.xml",
            f"{base_path}/primary_doc.xml",
            f"{base_path}/{accession}.xml"
        ])
        
        # æ–¹æ³•3: åŸºäºç›®å½•çš„å…¶ä»–å¯èƒ½æ€§
        candidates.extend([
            f"{base_path}/doc4.xml",
            f"{base_path}/ownership.xml"
        ])
        
        # å»é‡
        return list(dict.fromkeys(candidates))

    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆ URL çš„å”¯ä¸€ç¼“å­˜é”®"""
        return hashlib.md5(url.encode()).hexdigest()

    def _is_valid_form4_xml(self, content: bytes) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ Form 4 XML"""
        if len(content) < 500:  # å¤ªçŸ­çš„å†…å®¹ä¸å¤ªå¯èƒ½æ˜¯æœ‰æ•ˆçš„ Form 4
            return False
        
        content_str = content.decode('utf-8', errors='ignore').lower()
        
        # æ£€æŸ¥å¿…è¦çš„ XML æ ‡ç­¾
        required_tags = ["<ownershipdocument>", "<issuer>"]
        for tag in required_tags:
            if tag not in content_str:
                return False
        
        # æ£€æŸ¥æ˜¯å¦ç¡®å®æ˜¯ Form 4
        form4_indicators = ["form4", "form 4", "ownershipdocument"]
        if not any(indicator in content_str for indicator in form4_indicators):
            return False
            
        return True
