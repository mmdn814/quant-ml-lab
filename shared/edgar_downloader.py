import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET # Import for XML validation


class EdgarDownloader:
    """
    ç®€åŒ–ç‰ˆ EdgarDownloaderï¼šç”¨äºä¸‹è½½ SEC Form 4 XML æ–‡ä»¶ã€‚
    
    âœ… ä»…ä¾èµ– <category term="4"> åˆ¤æ–­æ˜¯å¦ä¸º Form 4
    âœ… ä¸å†éªŒè¯ XML å†…å®¹ç»“æ„
    âœ… æ”¯æŒ index é¡µé¢è§£æ å’Œ fallback è·¯å¾„æ‹¼æ¥ä¸‹è½½
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ– downloader å¯¹è±¡

        Args:
            logger: æ—¥å¿—è®°å½•å™¨å¯¹è±¡
            max_retries: æ¯ä¸ªé“¾æ¥æœ€å¤šé‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡ HTTP è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            request_interval: æ¯ä¸ªæ¡ç›®ä¸‹è½½ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
            base_url: SEC ä¸»ç«™ URL
            cache_dir: ä¸‹è½½ç¼“å­˜ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval

        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 14, save_dir: str = "data/form4") -> List[str]:
        """
        ä¸»å‡½æ•°ï¼šä¸‹è½½æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š
        
        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: XML æ–‡ä»¶ä¿å­˜ç›®å½•

        Returns:
            æ‰€æœ‰æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        # Log the total number of entries to process
        self.logger.info(f"ğŸ“¥ å³å°†ä¸‹è½½ Form 4 æ–‡ä»¶æ•°: {len(entries)}")

        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæˆ–æ— æ³•ä¸‹è½½çš„æ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}", exc_info=True) # Print detailed stack trace
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        Constructs the Atom Feed request URL to get Form 4 reports from the last N days.

        Args:
            days_back: Number of days to look back.

        Returns:
            Full URL string.
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",  # Form 4 type
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """
        Requests the Atom Feed and filters out Form 4 reports with <category term="4">.

        Args:
            url: Feed page URL.

        Returns:
            List of all Form 4 type entries.
        """
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status() # Check for HTTP errors
            soup = BeautifulSoup(response.content, "xml")
            entries = soup.find_all("entry")

            # âœ… Only keep entries with <category term="4">
            form4_entries = [e for e in entries if e.find("category", {"term": "4"})]
            self.logger.info(f"ğŸ¯ å…± {len(entries)} ä¸ªæ¡ç›®ï¼Œç­›é€‰å‡º {len(form4_entries)} ä¸ª Form 4")
            return form4_entries
        except requests.exceptions.RequestException as e:
            self.logger.error(f"åŠ è½½ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []
        except Exception as e:
            self.logger.error(f"è§£æ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """
        Processes a single Form 4 entry, downloads and saves the XML file.

        Args:
            entry: Single <entry> node.
            save_dir: Local save directory.

        Returns:
            Path to the successfully saved file, or None.
        """
        filing_url = entry.link["href"]
        try:
            cik, accession = self._extract_identifiers(filing_url)
        except ValueError as e:
            self.logger.warning(f"ğŸš« æ— æ³•ä» filing URL æå–æ ‡è¯†ç¬¦ï¼Œè·³è¿‡: {filing_url}ï¼Œé”™è¯¯: {e}")
            return None

        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        if os.path.exists(filepath):
            # This can be extended to re-download if the cached file is found invalid
            # For now, it skips if the file exists, as the primary validation is during download
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥ï¼Œæ— æ³•è·å– XML å†…å®¹: {filing_url}")
            return None

        try:
            with open(filepath, "wb") as f:
                f.write(content)
            return filepath
        except IOError as e:
            self.logger.error(f"ğŸš« å†™å…¥æ–‡ä»¶å¤±è´¥: {filepath}ï¼Œé”™è¯¯: {e}")
            return None

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """
        Attempts to download the XML file from multiple URLs (index page + fallback paths).

        Returns:
            Downloaded XML content, or None.
        """
        # First, try to get XML URLs by parsing the index page
        xml_urls_from_index = self._get_xml_urls_from_index(filing_url)
        if xml_urls_from_index:
            self.logger.debug(f"ä»ç´¢å¼•é¡µ {filing_url} æå–åˆ° XML é“¾æ¥: {xml_urls_from_index}")
            # Try these extracted URLs first
            for url in xml_urls_from_index:
                content = self._try_download(url)
                if content:
                    return content
        else:
            self.logger.debug(f"æœªèƒ½ä»ç´¢å¼•é¡µ {filing_url} æå–åˆ°æœ‰æ•ˆ XML é“¾æ¥ã€‚")

        # If index page parsing failed or didn't yield a downloadable file, use generated candidates
        generated_xml_urls = self._generate_candidate_urls(filing_url, cik, accession)
        if generated_xml_urls:
            self.logger.debug(f"ç”Ÿæˆäº†å€™é€‰ XML é“¾æ¥: {generated_xml_urls}")
            for url in generated_xml_urls:
                content = self._try_download(url)
                if content:
                    return content
        else:
            self.logger.warning(f"æœªèƒ½ä¸º {filing_url} ç”Ÿæˆä»»ä½•å€™é€‰ XML ä¸‹è½½é“¾æ¥ã€‚")

        return None

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        å°è¯•è§£æ index é¡µé¢ï¼Œæå– XML æ–‡ä»¶çœŸå®è·¯å¾„ã€‚
        ä¿®æ”¹ï¼šç¡®ä¿ä»ç´¢å¼•é¡µè·å–çš„XML URLç›´æ¥ä½äºå½’æ¡£è·¯å¾„ä¸‹ï¼Œ
        å»é™¤å¯èƒ½å­˜åœ¨çš„å­ç›®å½•è·¯å¾„ï¼Œå¦‚ "xslF345X05/"ã€‚

        Returns:
            æ‰€æœ‰æ‰¾åˆ°çš„ XML ä¸‹è½½é“¾æ¥
        """
        try:
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                self.logger.debug(f"è®¿é—®ç´¢å¼•é¡µå¤±è´¥: {filing_url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, "html.parser") 
            xml_urls = []

            # 1. ä» filing_url ä¸­æå– CIK å’Œ Accession Numberï¼Œä»¥æ„å»ºé¢„æœŸçš„æ ‡å‡†å½’æ¡£è·¯å¾„
            # Example filing_url: https://www.sec.gov/Archives/edgar/data/17313/000001731325000061/0000017313-25-000061-index.htm
            try:
                cik, accession = self._extract_identifiers(filing_url)
                # Construct the direct base path for the XML files
                # This should be the path where primary XMLs usually reside
                expected_base_archive_path = urljoin(self.base_url, 
                                                     f"/Archives/edgar/data/{int(cik)}/{accession}/")
                self.logger.debug(f"æ„å»ºé¢„æœŸå½’æ¡£åŸºè·¯å¾„: {expected_base_archive_path}")
            except ValueError:
                self.logger.warning(f"æ— æ³•ä» filing URL {filing_url} æå– CIK å’Œ Accessionï¼Œæ— æ³•ç¡®å®šé¢„æœŸXMLåŸºè·¯å¾„ã€‚è·³è¿‡æ­¤ç´¢å¼•é¡µè§£æã€‚")
                return [] # If identifiers can't be extracted, we can't reliably form the base path.


            for link in soup.find_all("a", href=True):
                href = link["href"]
                # Look for links ending in .xml
                if href.lower().endswith(".xml"):
                    # Extract only the base filename from the href (e.g., "doc4.xml" from "xslF345X05/doc4.xml")
                    xml_filename = os.path.basename(href) 

                    # Construct the full URL by joining the base archive path with the extracted filename.
                    # This ensures the XML is always sought directly under the CIK/Accession directory.
                    full_url = urljoin(expected_base_archive_path, xml_filename)
                    xml_urls.append(full_url)
                    self.logger.debug(f"ç´¢å¼•é¡µæ‰¾åˆ°XMLé“¾æ¥: {full_url}")

            # Use dict.fromkeys to remove duplicates and convert back to list
            return list(dict.fromkeys(xml_urls))
        except requests.exceptions.RequestException as e:
            self.logger.debug(f"å°è¯•è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.debug(f"è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        Extracts CIK and accession number from the URL.
        
        Raises:
            ValueError: If the format does not match.
        """
        # Modified regex to more precisely match the accession number, especially before the final '-index.htm'
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url) 
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        cik = match.group(1).zfill(10) # Ensure CIK is 10 digits, padded with leading zeros
        # Accession number is usually 18 digits. Remove non-digit and non-hyphen characters,
        # but preserve hyphens for the original structure to match file naming conventions.
        # SEC's accession number format is YYYYMMDD-XXXXXX-XXXXX, here we retain hyphens.
        accession_raw = match.group(2)
        accession = re.sub(r'[^0-9\-]', '', accession_raw) 
        # Example: 0001127602-25-017854
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        åœ¨æ— æ³•è§£æ index é¡µæ—¶ï¼Œæ‹¼å‡ºæ‰€æœ‰å¸¸è§ Form 4 XML æ–‡ä»¶åä½œä¸ºå€™é€‰é“¾æ¥
        ä¼˜åŒ–å°è¯•é¡ºåºï¼Œä¼˜å…ˆå°è¯•æ›´å¸¸è§çš„åç§°ã€‚
        """
        clean_accession = ''.join(c for c in accession if c.isdigit())
        # Ensure trailing slash for urljoin
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}/" 
        
        candidate_urls = [
            # 1. Most common, explicit names (prioritize these based on observation)
            f"{base_path}primary_doc.xml",
            f"{base_path}form4.xml",
            f"{base_path}doc4.xml",
            f"{base_path}ownership.xml", # Added based on user feedback

            # 2. Common SEC-generated filename patterns involving the clean accession
            f"{base_path}wk-form4_{clean_accession}.xml",
            f"{base_path}xslForm4_{clean_accession}.xml",
            f"{base_path}nc-form4_{clean_accession}.xml",
            f"{base_path}e{clean_accession}.xml",

            # 3. Direct accession number as filename (less common for Form4 primary XMLs, but possible)
            f"{base_path}{accession}.xml", # Original accession with hyphens
            f"{base_path}{clean_accession}.xml", # Purely numeric accession

            # 4. Fallback for simple replacement of index.htm with .xml (this often generates ACCESSION-YY-XXXXXX.xml)
            # This is less reliable but kept as a last resort.
            filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"),
        ]
        # Use dict.fromkeys to remove duplicates and preserve order
        return list(dict.fromkeys(candidate_urls))

    def _try_download(self, url: str) -> Optional[bytes]:
        """
        å•é“¾æ¥ä¸‹è½½ï¼Œæ”¯æŒç¼“å­˜å’Œæœ€å¤š N æ¬¡é‡è¯•
        æ–°å¢ XML å†…å®¹æœ‰æ•ˆæ€§æ ¡éªŒã€‚

        Returns:
            æˆåŠŸçš„äºŒè¿›åˆ¶å†…å®¹æˆ– None
        """
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_path = os.path.join(self.cache_dir, cache_key)

        # Check cache, if it exists and is valid, return directly
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                content = f.read()
                try:
                    ET.fromstring(content) # Attempt to parse, ensure cached file is valid XML
                    self.logger.debug(f"ä»ç¼“å­˜åŠ è½½å¹¶éªŒè¯æˆåŠŸ: {os.path.basename(url)}")
                    return content
                except ET.ParseError:
                    self.logger.warning(f"ç¼“å­˜æ–‡ä»¶ {os.path.basename(url)} XML æ ¼å¼æ— æ•ˆï¼Œå°è¯•é‡æ–°ä¸‹è½½ã€‚")
                    os.remove(cache_path) # Delete invalid cached file
                except Exception as e: # Catch other errors during cache read/validation
                    self.logger.warning(f"è¯»å–æˆ–éªŒè¯ç¼“å­˜æ–‡ä»¶ {os.path.basename(url)} å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°ä¸‹è½½ã€‚")
                    os.remove(cache_path)
                    
        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"å°è¯•ä¸‹è½½: {url} (å°è¯• {attempt + 1}/{self.max_retries})")
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status() # Check HTTP status code (4xx, 5xx)

                content = response.content
                # !!! Key modification: Validate XML content before saving !!!
                try:
                    ET.fromstring(content) # Attempt to parse downloaded content, raises ParseError if not valid XML
                except ET.ParseError:
                    self.logger.warning(f"ä¸‹è½½æ–‡ä»¶ {url} XML æ ¼å¼æ— æ•ˆï¼ˆéè‰¯å¥½æ ¼å¼XMLï¼‰ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                    time.sleep(min(2 ** (attempt + 1), 10)) # Exponential backoff, starting from 1 second
                    continue # Skip this loop, go to next retry

                # If XML content is valid, save to cache
                with open(cache_path, "wb") as f:
                    f.write(content)
                self.logger.debug(f"æˆåŠŸä¸‹è½½å¹¶éªŒè¯: {url}")
                return content
            except requests.exceptions.RequestException as e: # Catch requests library errors (network issues, timeouts, HTTP errors, etc.)
                self.logger.warning(f"ä¸‹è½½ {url} å¤±è´¥: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                time.sleep(min(2 ** (attempt + 1), 10)) # Exponential backoff, starting from 1 second
            except Exception as e: # Catch other unknown errors
                self.logger.error(f"ä¸‹è½½ {url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})", exc_info=True)
                time.sleep(min(2 ** (attempt + 1), 10))
        
        self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•æˆåŠŸä¸‹è½½å¹¶éªŒè¯ {url}")
        return None

