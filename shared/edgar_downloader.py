import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin
import xml.etree.ElementTree as ET # å¯¼å…¥ElementTree

import requests
from bs4 import BeautifulSoup


class EdgarDownloader:
    """
    ç®€åŒ–ç‰ˆ EdgarDownloaderï¼šç”¨äºä¸‹è½½ SEC Form 4 XML æ–‡ä»¶ã€‚
    
    âœ… ä»…ä¾èµ– <category term="4"> åˆ¤æ–­æ˜¯å¦ä¸º Form 4
    âœ… ä¸å†éªŒè¯ XML å†…å®¹ç»“æ„
    âœ… æ”¯æŒ index é¡µé¢è§£æ å’Œ fallback è·¯å¾„æ‹¼æ¥ä¸‹è½½
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ– downloader å¯¹è±¡

        Args:
            logger: æ—¥å¿—è®°å½•å™¨å¯¹è±¡
            max_retries: æ¯ä¸ªé“¾æ¥æœ€å¤šé‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡ HTTP è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            request_interval: æ¯ä¸ªæ¡ç›®ä¸‹è½½ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
            base_url: SEC ä¸»ç«™ URL
            cache_dir: ä¸‹è½½ç¼“å­˜ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval

        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 7, save_dir: str = "data/form4") -> List[str]:
        """
        ä¸»å‡½æ•°ï¼šä¸‹è½½æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š
        
        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: XML æ–‡ä»¶ä¿å­˜ç›®å½•

        Returns:
            æ‰€æœ‰æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        # æ—¥å¿—è¾“å‡ºï¼šæ€»å…±è¦å¤„ç†å¤šå°‘ä¸ªæ¡ç›®
        self.logger.info(f"ğŸ“¥ å³å°†ä¸‹è½½ Form 4 æ–‡ä»¶æ•°: {len(entries)}")

        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæˆ–æ— æ³•ä¸‹è½½çš„æ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}", exc_info=True) # æ‰“å°è¯¦ç»†å †æ ˆ
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        æ„é€  Atom Feed è¯·æ±‚é“¾æ¥ï¼Œç”¨äºè·å–æœ€è¿‘ N å¤©å†…çš„ Form 4 æŠ¥å‘Š

        Args:
            days_back: å›æº¯å¤©æ•°

        Returns:
            å®Œæ•´ URL å­—ç¬¦ä¸²
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",  # Form 4 ç±»å‹
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """
        è¯·æ±‚ Atom Feed å¹¶ç­›é€‰å‡º <category term="4"> çš„ Form 4 æŠ¥å‘Š

        Args:
            url: Feed é¡µé¢ URL

        Returns:
            æ‰€æœ‰ Form 4 ç±»å‹çš„ entry åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status() # æ£€æŸ¥HTTPé”™è¯¯
            soup = BeautifulSoup(response.content, "xml")
            entries = soup.find_all("entry")

            # âœ… åªä¿ç•™ <category term="4"> çš„æ¡ç›®
            form4_entries = [e for e in entries if e.find("category", {"term": "4"})]
            self.logger.info(f"ğŸ¯ å…± {len(entries)} ä¸ªæ¡ç›®ï¼Œç­›é€‰å‡º {len(form4_entries)} ä¸ª Form 4")
            return form4_entries
        except requests.exceptions.RequestException as e:
            self.logger.error(f"åŠ è½½ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []
        except Exception as e:
            self.logger.error(f"è§£æ Feed å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}", exc_info=True)
            return []

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """
        å¤„ç†å•ä¸ª Form 4 entryï¼Œä¸‹è½½å¹¶ä¿å­˜ XML æ–‡ä»¶

        Args:
            entry: å•ä¸ª <entry> èŠ‚ç‚¹
            save_dir: æœ¬åœ°ä¿å­˜ç›®å½•

        Returns:
            æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ– None
        """
        filing_url = entry.link["href"]
        try:
            cik, accession = self._extract_identifiers(filing_url)
        except ValueError as e:
            self.logger.warning(f"ğŸš« æ— æ³•ä» filing URL æå–æ ‡è¯†ç¬¦ï¼Œè·³è¿‡: {filing_url}ï¼Œé”™è¯¯: {e}")
            return None

        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        if os.path.exists(filepath):
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä¸€ä¸ªé€‰é¡¹ï¼šå¦‚æœæ–‡ä»¶å­˜åœ¨ä½†æ ¡éªŒå¤±è´¥ï¼Œåˆ™é‡æ–°ä¸‹è½½
            # ç›®å‰ä¿æŒè·³è¿‡ï¼Œå› ä¸ºé—®é¢˜ä¸»è¦åœ¨ä¸‹è½½é˜¶æ®µçš„æ ¡éªŒ
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥ï¼Œæ— æ³•è·å– XML å†…å®¹: {filing_url}")
            return None

        try:
            with open(filepath, "wb") as f:
                f.write(content)
            return filepath
        except IOError as e:
            self.logger.error(f"ğŸš« å†™å…¥æ–‡ä»¶å¤±è´¥: {filepath}ï¼Œé”™è¯¯: {e}")
            return None

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """
        å°è¯•å¤šç§ URL ä¸‹è½½ XML æ–‡ä»¶ï¼ˆindex é¡µé¢ + fallback è·¯å¾„ï¼‰

        Returns:
            ä¸‹è½½åˆ°çš„ XML å†…å®¹ï¼Œæˆ– None
        """
        xml_urls = self._get_xml_urls_from_index(filing_url)
        if not xml_urls:
            # å¦‚æœä»ç´¢å¼•é¡µé¢æœªèƒ½æå–åˆ°ä»»ä½•XML URLï¼Œåˆ™ç”Ÿæˆå¤‡é€‰URL
            xml_urls = self._generate_candidate_urls(filing_url, cik, accession)

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªURLå¯ä»¥å°è¯•ï¼Œå³ä½¿fallbackä¹Ÿå¯èƒ½ä¸ºç©º
        if not xml_urls:
            self.logger.warning(f"æ— æ³•ä¸º {filing_url} ç”Ÿæˆä»»ä½•å€™é€‰ XML ä¸‹è½½é“¾æ¥ã€‚")
            return None

        for url in xml_urls:
            content = self._try_download(url)
            if content:
                return content
        return None

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        å°è¯•è§£æ index é¡µé¢ï¼Œæå– XML æ–‡ä»¶çœŸå®è·¯å¾„

        Returns:
            æ‰€æœ‰æ‰¾åˆ°çš„ XML ä¸‹è½½é“¾æ¥
        """
        try:
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                self.logger.debug(f"è®¿é—®ç´¢å¼•é¡µå¤±è´¥: {filing_url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
            
            # ä½¿ç”¨html.parseræ›´é€‚åˆå¤„ç†å¯èƒ½ä¸è§„èŒƒçš„HTMLé¡µé¢
            soup = BeautifulSoup(response.content, "html.parser") 
            xml_urls = []

            for link in soup.find_all("a", href=True):
                href = link["href"]
                # å¯»æ‰¾ä»¥ .xml ç»“å°¾çš„é“¾æ¥
                if href.lower().endswith(".xml"): 
                    if href.startswith("/"):
                        full_url = self.base_url + href
                    else:
                        full_url = urljoin(filing_url, href)
                    xml_urls.append(full_url)

            # ä½¿ç”¨ dict.fromkeys å»é‡ï¼Œå¹¶è½¬å›åˆ—è¡¨
            return list(dict.fromkeys(xml_urls))
        except requests.exceptions.RequestException as e:
            self.logger.debug(f"å°è¯•è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.debug(f"è§£æç´¢å¼•é¡µ {filing_url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        ä» URL ä¸­æå– CIK å’Œ accession number
        
        Raises:
            ValueError: å¦‚æœæ ¼å¼ä¸åŒ¹é…
        """
        # ä¿®æ­£åŒ¹é…æ¨¡å¼ä»¥æ›´ç²¾ç¡®åœ°åŒ¹é…accession numberï¼Œç‰¹åˆ«æ˜¯å¤„ç†æœ€åçš„'-'
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url) 
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        cik = match.group(1).zfill(10) # ç¡®ä¿CIKæ˜¯10ä½ï¼Œå‰é¢è¡¥0
        # accession numberé€šå¸¸æ˜¯18ä½æ•°å­—ï¼Œå»æ‰å¯èƒ½å­˜åœ¨çš„çŸ­æ¨ªçº¿ï¼Œä½†ä¿ç•™æœ€åçš„-index.htmä¹‹å‰çš„-
        accession_raw = match.group(2)
        # ç§»é™¤éæ•°å­—å’ŒçŸ­æ¨ªçº¿ä¹‹å¤–çš„å­—ç¬¦ï¼Œä¿ç•™åŸå§‹ç»“æ„ä»¥åŒ¹é…æ–‡ä»¶å‘½åçº¦å®š
        # SECçš„accession numberæ ¼å¼æ˜¯YYYYMMDD-XXXXXX-XXXXXï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™çŸ­æ¨ªçº¿
        accession = re.sub(r'[^0-9\-]', '', accession_raw) 
        # ä¾‹å¦‚ 0001127602-25-017854
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        åœ¨æ— æ³•è§£æ index é¡µæ—¶ï¼Œæ‹¼å‡ºæ‰€æœ‰å¸¸è§ Form 4 XML æ–‡ä»¶åä½œä¸ºå€™é€‰é“¾æ¥
        """
        # æ¸…ç†accession numberï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ç”¨äºæ–‡ä»¶åæ‹¼æ¥
        clean_accession = ''.join(c for c in accession if c.isdigit())
        # æ„å»ºåŸºç¡€è·¯å¾„ï¼Œæ³¨æ„CIKè½¬æ¢ä¸ºintå»é™¤å‰å¯¼é›¶ï¼Œå†è½¬å›å­—ç¬¦ä¸²
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}"
        
        candidate_urls = [
            # 1. å°è¯•ç›´æ¥å°†ç´¢å¼•é¡µURLçš„-index.htm/.htmlæ›¿æ¢ä¸º.xml
            filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"),
            # 2. å¸¸è§çš„é€šç”¨XMLæ–‡ä»¶å
            f"{base_path}/primary_doc.xml",
            f"{base_path}/form4.xml",
            f"{base_path}/doc4.xml",
            # 3. ä»¥accession numberä½œä¸ºæ–‡ä»¶åçš„XML
            f"{base_path}/{accession}.xml", # åŸå§‹çš„åŒ…å«çŸ­æ¨ªçº¿çš„accession
            f"{base_path}/{clean_accession}.xml", # çº¯æ•°å­—çš„accession
            # 4. å¸¸è§çš„SECç”Ÿæˆæ–‡ä»¶åæ¨¡å¼
            f"{base_path}/wf-form4_{clean_accession}.xml",
            f"{base_path}/xslForm4_{clean_accession}.xml",
            f"{base_path}/nc-form4_{clean_accession}.xml", # å¦ä¸€ç§å¸¸è§æ¨¡å¼
            f"{base_path}/e{clean_accession}.xml" # è¿˜æœ‰ä»¥eå¼€å¤´çš„æ¨¡å¼
        ]
        # ä½¿ç”¨ dict.fromkeys å»é‡å¹¶ä¿æŒé¡ºåº
        return list(dict.fromkeys(candidate_urls))

    def _try_download(self, url: str) -> Optional[bytes]:
        """
        å•é“¾æ¥ä¸‹è½½ï¼Œæ”¯æŒç¼“å­˜å’Œæœ€å¤š N æ¬¡é‡è¯•
        æ–°å¢ XML å†…å®¹æœ‰æ•ˆæ€§æ ¡éªŒã€‚

        Returns:
            æˆåŠŸçš„äºŒè¿›åˆ¶å†…å®¹æˆ– None
        """
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_path = os.path.join(self.cache_dir, cache_key)

        # æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™ç›´æ¥è¿”å›
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                content = f.read()
                try:
                    ET.fromstring(content) # å°è¯•è§£æï¼Œç¡®ä¿ç¼“å­˜æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„XML
                    self.logger.debug(f"ä»ç¼“å­˜åŠ è½½å¹¶éªŒè¯æˆåŠŸ: {os.path.basename(url)}")
                    return content
                except ET.ParseError:
                    self.logger.warning(f"ç¼“å­˜æ–‡ä»¶ {os.path.basename(url)} XML æ ¼å¼æ— æ•ˆï¼Œå°è¯•é‡æ–°ä¸‹è½½ã€‚")
                    os.remove(cache_path) # åˆ é™¤æ— æ•ˆç¼“å­˜æ–‡ä»¶

        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç  (4xx, 5xx)

                content = response.content
                # !!! å…³é”®ä¿®æ”¹ï¼šåœ¨ä¿å­˜å‰æ ¡éªŒ XML å†…å®¹ !!!
                try:
                    ET.fromstring(content) # å°è¯•è§£æä¸‹è½½åˆ°çš„å†…å®¹ï¼Œå¦‚æœä¸æ˜¯æœ‰æ•ˆXMLä¼šæŠ›å‡ºParseError
                except ET.ParseError:
                    self.logger.warning(f"ä¸‹è½½æ–‡ä»¶ {url} XML æ ¼å¼æ— æ•ˆï¼ˆéè‰¯å¥½æ ¼å¼XMLï¼‰ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                    time.sleep(min(2 ** (attempt + 1), 10)) # æŒ‡æ•°é€€é¿ï¼Œä»1ç§’å¼€å§‹
                    continue # è·³è¿‡æœ¬æ¬¡å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•

                # å¦‚æœXMLå†…å®¹æœ‰æ•ˆï¼Œåˆ™ä¿å­˜åˆ°ç¼“å­˜
                with open(cache_path, "wb") as f:
                    f.write(content)
                return content
            except requests.exceptions.RequestException as e: # æ•è· requests åº“çš„é”™è¯¯ï¼ˆç½‘ç»œé—®é¢˜ã€è¶…æ—¶ã€HTTPé”™è¯¯ç­‰ï¼‰
                self.logger.warning(f"ä¸‹è½½ {url} å¤±è´¥: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})")
                time.sleep(min(2 ** (attempt + 1), 10)) # æŒ‡æ•°é€€é¿ï¼Œä»1ç§’å¼€å§‹
            except Exception as e: # æ•è·å…¶ä»–æœªçŸ¥é”™è¯¯
                self.logger.error(f"ä¸‹è½½ {url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ï¼Œå°è¯•é‡è¯• ({attempt + 1}/{self.max_retries})", exc_info=True)
                time.sleep(min(2 ** (attempt + 1), 10))
        
        self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•æˆåŠŸä¸‹è½½å¹¶éªŒè¯ {url}")
        return None

