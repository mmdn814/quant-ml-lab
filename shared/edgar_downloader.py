import os
import re
import requests
import time
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from urllib.parse import urlparse

class EdgarDownloader:
    def __init__(self, logger, max_retries=3):
        self.logger = logger
        self.max_retries = max_retries
        self.headers = {
            "User-Agent": "QuantMLLabBot/1.0 (Contact: mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml"
        }
        self.request_interval = 0.5  # SECæ¨èæœ€å°é—´éš”

    def download_latest_form4(self, days_back=3):
        """
        ä¸‹è½½æœ€è¿‘Nå¤©çš„Form 4ç”³æŠ¥æ–‡ä»¶
        """
        save_dir = os.path.abspath("data/form4")
        os.makedirs(save_dir, exist_ok=True)
        
        self.logger.info(f"ğŸš€ è·å–æœ€è¿‘ {days_back} å¤©çš„Form 4ç”³æŠ¥")
        downloaded_files = []

        try:
            feed_url = self._build_feed_url(days_back)
            entries = self._fetch_atom_feed(feed_url)
            if not entries:
                self.logger.warning("âŒ æœªè·å–åˆ°ä»»ä½•æ¡ç›®")
                return []

            for idx, entry in enumerate(entries, 1):
                try:
                    filepath = self._process_entry(entry, save_dir)
                    if filepath:
                        downloaded_files.append(filepath)
                        self.logger.debug(f"[{idx}/{len(entries)}] æˆåŠŸä¸‹è½½")
                except Exception as e:
                    self.logger.error(f"å¤„ç†æ¡ç›®å¤±è´¥: {e}", exc_info=True)
                finally:
                    time.sleep(self.request_interval)

        except Exception as e:
            self.logger.critical(f"ä¸»æµç¨‹å¼‚å¸¸: {e}", exc_info=True)
        
        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back):
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=days_back)
        return (
            "https://www.sec.gov/cgi-bin/browse-edgar"
            f"?action=getcurrent&type=4"
            f"&datea={start_date.strftime('%Y%m%d')}"
            "&output=atom"
        )

    def _fetch_atom_feed(self, url):
        try:
            resp = requests.get(url, headers=self.headers, timeout=15)
            resp.raise_for_status()

            if "xml" not in resp.headers.get("Content-Type", ""):
                raise ValueError("éXMLå“åº”ï¼Œå¯èƒ½è¢«SECé™åˆ¶")

            soup = BeautifulSoup(resp.text, "xml")
            return soup.find_all("entry")
        except Exception as e:
            self.logger.error(f"âŒ è·å– Atom Feed å¤±è´¥: {e}")
            raise

    def _process_entry(self, entry, save_dir):
        if not entry.link:
            raise ValueError("âŒ æ¡ç›®ç¼ºå°‘ link æ ‡ç­¾")

        filing_url = entry.link["href"]
        company_name = entry.find("title").text.split(" - ")[0]
        updated = entry.find("updated").text if entry.find("updated") else "N/A"

        xml_url = self._normalize_xml_url(filing_url)
        if not xml_url:
            raise ValueError("âŒ æ— æ³•ç”Ÿæˆ XML é“¾æ¥")

        cik, accession = self._extract_identifiers(xml_url)
        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        self.logger.info(f"ğŸ“¥ ä¸‹è½½ {company_name} (CIK:{cik}) æ–‡ä»¶")
        content = self._download_with_retry(xml_url)
        if content:
            with open(filepath, "wb") as f:
                f.write(content)
            return filepath
        return None

    def _normalize_xml_url(self, filing_url):
        base_url = filing_url.replace("-index.htm", "").replace("-index.html", "")

        for ext in ["/primary_doc.xml", ".xml", "/index.xml"]:
            test_url = base_url + ext
            if self._validate_url(test_url):
                return test_url
        return None

    def _validate_url(self, url):
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc, result.path])
        except:
            return False

    def _extract_identifiers(self, url):
        """
        æå– CIK å’Œ Accession ç¼–å·ï¼Œå¤„ç†å¦‚ï¼š
        https://www.sec.gov/Archives/edgar/data/9631/000183988225034027/0001839882-25-034027/primary_doc.xml
        """
        try:
            match = re.search(r"/data/(\d{1,10})/(\d{18})/", url)
            if not match:
                raise ValueError("URL æ ¼å¼ä¸ç¬¦ï¼Œæ— æ³•æå–æ ‡è¯†ç¬¦")

            cik_raw, accession = match.groups()
            cik = cik_raw.zfill(10)

            if not re.fullmatch(r"\d{18}", accession):
                raise ValueError(f"Accession æ ¼å¼éæ³•: {accession}")

            return cik, accession
        except Exception as e:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}ï¼Œé”™è¯¯: {str(e)}")

    def _download_with_retry(self, url):
        for attempt in range(self.max_retries):
            try:
                resp = requests.get(url, headers=self.headers, timeout=15, stream=True)
                resp.raise_for_status()

                if resp.status_code == 200 and "xml" in resp.headers.get("Content-Type", "").lower():
                    return resp.content
            except requests.RequestException as e:
                if attempt < self.max_retries - 1:
                    wait = 2 ** attempt
                    self.logger.warning(f"ç¬¬ {attempt+1} æ¬¡é‡è¯•å¤±è´¥: {e}ï¼Œç­‰å¾… {wait}s")
                    time.sleep(wait)
                else:
                    self.logger.error(f"ğŸš« ä¸‹è½½å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return None
