# âœ… ä¿®å¤åçš„ `edgar_downloader.py` - è§£å†³ XML æ–‡ä»¶å®šä½é—®é¢˜/claude
# ä¸»è¦ä¿®å¤ï¼š
# 1. æ”¹è¿› XML æ–‡ä»¶ URL æ„é€ ç­–ç•¥
# 2. å…ˆè§£æ index é¡µé¢è·å–å®é™…çš„ XML æ–‡ä»¶å
# 3. å¢åŠ æ›´å¤šå€™é€‰ URL æ¨¡å¼
# 4. æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
# æœ€åä¿®æ”¹æ—¶é—´: 6/27/25

import os
import re
import time
import hashlib
from typing import List, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import requests
from bs4 import BeautifulSoup

class EdgarDownloader:
    """
    ç”¨äºä» SEC EDGAR Atom Feed ä¸‹è½½æœ€è¿‘ Form 4 æŠ¥å‘Šçš„æ ¸å¿ƒç±»ã€‚
    æ”¯æŒè‡ªåŠ¨ URL ä¿®å¤ã€å†…å®¹éªŒè¯ã€é‡è¯•æœºåˆ¶ä¸ç¼“å­˜ç­–ç•¥ã€‚
    """

    def __init__(
        self,
        logger,
        max_retries: int = 3,
        timeout: int = 15,
        request_interval: float = 0.5,
        base_url: str = "https://www.sec.gov",
        cache_dir: str = ".cache"
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: æ¯æ¬¡è¯·æ±‚è¶…æ—¶
            request_interval: è¯·æ±‚ä¹‹é—´çš„é—´éš”ç§’æ•°
            base_url: SEC åŸºç¡€åœ°å€
            cache_dir: ç”¨äºå­˜å‚¨ç¼“å­˜çš„ç›®å½•
        """
        self.logger = logger
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_url = base_url
        self.request_interval = request_interval
        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)",
            "Accept": "application/xml, text/xml, text/html"
        }

        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def download_latest_form4(self, days_back: int = 7, save_dir: str = "data/form4", max_workers: int = 1) -> List[str]:
        """
        ä¸‹è½½æœ€è¿‘ N å¤©çš„ Form 4 XML æ–‡ä»¶

        Args:
            days_back: å›æº¯å¤©æ•°
            save_dir: æœ¬åœ°ä¿å­˜ç›®å½•
            max_workers: å¹¶å‘æ•°ï¼ˆæš‚æœªä½¿ç”¨ï¼‰

        Returns:
            ä¸‹è½½æˆåŠŸçš„ XML æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(save_dir, exist_ok=True)
        feed_url = self._build_feed_url(days_back)
        entries = self._fetch_atom_feed(feed_url)

        downloaded_files = []
        for idx, entry in enumerate(entries, 1):
            try:
                filepath = self._process_entry(entry, save_dir)
                if filepath:
                    downloaded_files.append(filepath)
                    self.logger.debug(f"[{idx}/{len(entries)}] ä¸‹è½½å®Œæˆ: {os.path.basename(filepath)}")
                else:
                    self.logger.warning(f"[{idx}/{len(entries)}] è·³è¿‡æ— æ•ˆæ¡ç›®")
            except Exception as e:
                self.logger.error(f"[{idx}/{len(entries)}] å¤„ç†æ¡ç›®å¤±è´¥: {e}")
            finally:
                time.sleep(self.request_interval)

        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)} / å…± {len(entries)} ä¸ªæ–‡ä»¶")
        return downloaded_files

    def _build_feed_url(self, days_back: int) -> str:
        """
        æ„é€  SEC Atom Feed URL

        Args:
            days_back: å›æº¯å¤©æ•°

        Returns:
            å®Œæ•´ URL
        """
        start_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        params = {
            "action": "getcurrent",
            "type": "4",
            "datea": start_date.strftime("%Y%m%d"),
            "output": "atom"
        }
        return urljoin(self.base_url, "/cgi-bin/browse-edgar?" + urlencode(params))

    def _fetch_atom_feed(self, url: str):
        """è¯·æ±‚å¹¶è§£æ Atom Feedï¼Œè¿‡æ»¤å‡ºçœŸæ­£çš„ Form 4 æ¡ç›®"""
        self.logger.info(f"ğŸ“¡ åŠ è½½ Feed: {url}")
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")
        all_entries = soup.find_all("entry")
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„ Form 4 æ¡ç›®
        form4_entries = []
        for entry in all_entries:
            try:
                # æ£€æŸ¥æ¡ç›®æ ‡é¢˜å’Œæ‘˜è¦æ˜¯å¦åŒ…å« Form 4 ç›¸å…³ä¿¡æ¯
                title = entry.title.get_text() if entry.title else ""
                summary = entry.summary.get_text() if entry.summary else ""
                
                # Form 4 çš„ç‰¹å¾ï¼šæ ‡é¢˜åŒ…å« "4"ï¼Œæ‘˜è¦åŒ…å« "Form 4" æˆ–ç›¸å…³å…³é”®è¯
                if self._is_form4_entry(title, summary):
                    form4_entries.append(entry)
                else:
                    self.logger.debug(f"è·³è¿‡é Form 4 æ¡ç›®: {title}")
            except Exception as e:
                self.logger.debug(f"è§£ææ¡ç›®æ—¶å‡ºé”™: {e}")
                continue
        
        self.logger.info(f"ä» {len(all_entries)} ä¸ªæ¡ç›®ä¸­ç­›é€‰å‡º {len(form4_entries)} ä¸ª Form 4 æ¡ç›®")
        return form4_entries

    def _process_entry(self, entry, save_dir: str) -> Optional[str]:
        """å¤„ç†å•ä¸ª Atom é¡¹ï¼Œå°è¯•ä¸‹è½½å¹¶ä¿å­˜å¯¹åº” Form 4 XML"""
        filing_url = entry.link["href"]
        cik, accession = self._extract_identifiers(filing_url)
        filename = f"{cik}_{accession}.xml"
        filepath = os.path.join(save_dir, filename)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(filepath):
            self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return filepath

        content = self._download_with_fallback(filing_url, cik, accession)
        if not content:
            self.logger.warning(f"ğŸš« æ‰€æœ‰ä¸‹è½½æ–¹å¼å¤±è´¥: {filing_url}")
            return None

        with open(filepath, "wb") as f:
            f.write(content)
        return filepath

    def _download_with_fallback(self, filing_url: str, cik: str, accession: str) -> Optional[bytes]:
        """å°è¯•å¤šç§æ–¹å¼ä¸‹è½½ Form 4 XML å†…å®¹"""
        self.logger.debug(f"å¼€å§‹ä¸‹è½½: CIK={cik}, Accession={accession}")
        
        # é¦–å…ˆå°è¯•ä» index é¡µé¢è§£æå®é™…çš„ XML æ–‡ä»¶å
        xml_urls = self._get_xml_urls_from_index(filing_url)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„å€™é€‰ URL æ–¹æ³•
        if not xml_urls:
            self.logger.debug("Index é¡µé¢è§£æå¤±è´¥ï¼Œä½¿ç”¨å€™é€‰ URL ç­–ç•¥")
            xml_urls = self._generate_candidate_urls(filing_url, cik, accession)
        else:
            self.logger.debug(f"ä» Index é¡µé¢è§£æå‡º {len(xml_urls)} ä¸ªå€™é€‰ URL")
        
        # è®°å½•æ‰€æœ‰å€™é€‰ URL
        for i, url in enumerate(xml_urls):
            self.logger.debug(f"å€™é€‰ URL {i+1}: {url}")
        
        for i, url in enumerate(xml_urls):
            self.logger.debug(f"å°è¯•å€™é€‰ URL {i+1}/{len(xml_urls)}: {url}")
            content = self._try_download(url)
            if content:
                self.logger.debug(f"âœ… æˆåŠŸä¸‹è½½: {url}")
                return content
            else:
                self.logger.debug(f"âŒ ä¸‹è½½å¤±è´¥: {url}")
        
        self.logger.debug(f"æ‰€æœ‰ {len(xml_urls)} ä¸ªå€™é€‰ URL å‡ä¸‹è½½å¤±è´¥")
        return None

    def _is_form4_entry(self, title: str, summary: str) -> bool:
        """
        åˆ¤æ–­ Atom Feed æ¡ç›®æ˜¯å¦ä¸º Form 4
        
        Args:
            title: æ¡ç›®æ ‡é¢˜
            summary: æ¡ç›®æ‘˜è¦
            
        Returns:
            æ˜¯å¦ä¸º Form 4 æ¡ç›®
        """
        title_lower = title.lower()
        summary_lower = summary.lower()
        
        # Form 4 çš„æ˜ç¡®æ ‡è¯†
        form4_indicators = [
            "form 4", "form4", "statement of changes in beneficial ownership"
        ]
        
        # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦
        for indicator in form4_indicators:
            if indicator in title_lower or indicator in summary_lower:
                return True
        
        # é¢å¤–æ£€æŸ¥ï¼šæ ‡é¢˜åŒ…å«æ•°å­— "4" ä¸”æ‘˜è¦åŒ…å«ç›¸å…³å…³é”®è¯
        if "4" in title and any(keyword in summary_lower for keyword in [
            "beneficial ownership", "insider", "section 16", "ownership"
        ]):
            return True
            
        return False

    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
    def _get_xml_urls_from_index(self, filing_url: str) -> List[str]:
        """
        ä» filing index é¡µé¢è§£æå‡ºå®é™…çš„ XML æ–‡ä»¶é“¾æ¥
        """
        try:
            self.logger.debug(f"è§£æ index é¡µé¢: {filing_url}")
            response = self.session.get(filing_url, timeout=self.timeout)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.content, "html.parser")
            xml_urls = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ–‡æ¡£è¡¨æ ¼ä¸­çš„ XML æ–‡ä»¶
            for table in soup.find_all("table"):
                for row in table.find_all("tr"):
                    cells = row.find_all(["td", "th"])
                    if len(cells) >= 3:  # é€šå¸¸æœ‰ Seq, Description, Document, Type, Size åˆ—
                        for cell in cells:
                            link = cell.find("a", href=True)
                            if link and link["href"].endswith(".xml"):
                                href = link["href"]
                                if href.startswith("/"):
                                    full_url = self.base_url + href
                                else:
                                    full_url = urljoin(filing_url, href)
                                xml_urls.append(full_url)
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰ XML é“¾æ¥
            for link in soup.find_all("a", href=True):
                href = link["href"]
                if href.endswith(".xml"):
                    if href.startswith("/"):
                        full_url = self.base_url + href
                    else:
                        full_url = urljoin(filing_url, href)
                    xml_urls.append(full_url)
            
            # å»é‡å¹¶ä¼˜å…ˆé€‰æ‹©ä¸»æ–‡æ¡£
            xml_urls = list(dict.fromkeys(xml_urls))
            
            # æ ¹æ®æ–‡ä»¶åæ’åºï¼Œä¼˜å…ˆé€‰æ‹©å¯èƒ½çš„ä¸»æ–‡æ¡£
            def get_priority(url):
                filename = url.split("/")[-1].lower()
                if "primary" in filename or "form4" in filename:
                    return 0
                elif filename.startswith("wf-form4") or filename.startswith("xslform4"):
                    return 1
                elif filename == "form4.xml":
                    return 2
                else:
                    return 3
            
            xml_urls.sort(key=get_priority)
            
            self.logger.debug(f"ä» index é¡µé¢æ‰¾åˆ° {len(xml_urls)} ä¸ª XML å€™é€‰")
            return xml_urls
            
        except Exception as e:
            self.logger.debug(f"è§£æ index é¡µé¢å¤±è´¥: {e}")
            return []

    def _try_download(self, url: str) -> Optional[bytes]:
        """æ‰§è¡Œå•æ¬¡ä¸‹è½½å°è¯•ï¼ŒåŒ…å«ç¼“å­˜æœºåˆ¶"""
        cache_key = self._get_cache_key(url)
        cache_path = os.path.join(self.cache_dir, cache_key)

        # ä¼˜å…ˆè¿”å›ç¼“å­˜
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                content = f.read()
                if self._is_valid_form4_xml(content):
                    return content

        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"å°è¯•ä¸‹è½½ [{attempt+1}/{self.max_retries}]: {url}")
                response = self.session.get(url, timeout=self.timeout)
                
                if response.status_code == 200:
                    content = response.content
                    if self._is_valid_form4_xml(content):
                        # ç¼“å­˜æœ‰æ•ˆå†…å®¹
                        with open(cache_path, "wb") as f:
                            f.write(content)
                        return content
                    else:
                        self.logger.debug(f"å†…å®¹éªŒè¯å¤±è´¥: {url}")
                else:
                    self.logger.debug(f"HTTP {response.status_code}: {url}")
                    
            except Exception as e:
                wait = min(2 ** attempt, 10)  # æœ€å¤§ç­‰å¾… 10 ç§’
                self.logger.debug(f"ä¸‹è½½å¼‚å¸¸ [{attempt+1}/{self.max_retries}] {url}ï¼š{e}")
                if attempt < self.max_retries - 1:
                    time.sleep(wait)
        
        return None

    def _extract_identifiers(self, url: str) -> tuple[str, str]:
        """
        ä» URL ä¸­æå– CIK å’Œ Accession Number

        Raises:
            ValueError: æ— æ³•è¯†åˆ«çš„ URL æ ¼å¼
        """
        match = re.search(r"data/(\d+)/([0-9\-]+)/?", url)
        if not match:
            raise ValueError(f"æ— æ³•ä»URLæå–æ ‡è¯†ç¬¦: {url}")
        
        cik = match.group(1).zfill(10)
        accession_raw = match.group(2)
        # æ¸…ç† accession numberï¼Œä¿ç•™æ•°å­—å’Œè¿å­—ç¬¦
        accession = re.sub(r'[^\d\-]', '', accession_raw)
        
        return cik, accession

    def _generate_candidate_urls(self, filing_url: str, cik: str, accession: str) -> List[str]:
        """
        æ„é€ å¤šä¸ªå€™é€‰ XML ä¸‹è½½åœ°å€ä»¥æå‡å®¹é”™æ€§
        """
        clean_accession = ''.join(c for c in accession if c.isdigit())
        base_path = f"{self.base_url}/Archives/edgar/data/{int(cik)}/{accession}"
        
        candidates = []
        
        # æ–¹æ³•1: ç®€å•æ›¿æ¢ index åç¼€
        candidates.append(filing_url.replace("-index.htm", ".xml").replace("-index.html", ".xml"))
        
        # æ–¹æ³•2: åŸºäºçœŸå®çš„ SEC æ–‡ä»¶ç»“æ„
        # ä» filing_url ä¸­æå–è·¯å¾„ä¿¡æ¯
        url_parts = filing_url.split('/')
        if len(url_parts) >= 2:
            accession_part = url_parts[-2]  # è·å– accession number éƒ¨åˆ†
            
            # å¸¸è§çš„ Form 4 XML æ–‡ä»¶åæ¨¡å¼
            candidates.extend([
                f"{base_path}/primary_doc.xml",
                f"{base_path}/form4.xml",
                f"{base_path}/{accession_part}.xml",
                f"{base_path}/wf-form4_{clean_accession}.xml",
                f"{base_path}/xslForm4_{clean_accession}.xml",
                f"{base_path}/doc4.xml"
            ])
        
        # æ–¹æ³•3: åŸºäº index æ–‡ä»¶åæ„é€ 
        index_filename = filing_url.split('/')[-1]
        if index_filename.endswith('-index.htm') or index_filename.endswith('-index.html'):
            # ç§»é™¤ -index åç¼€ï¼Œæ·»åŠ  .xml
            base_filename = index_filename.replace('-index.htm', '').replace('-index.html', '')
            candidates.append(f"{base_path}/{base_filename}.xml")
        
        # å»é‡
        return list(dict.fromkeys(candidates))

    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆ URL çš„å”¯ä¸€ç¼“å­˜é”®"""
        return hashlib.md5(url.encode()).hexdigest()

    def _is_valid_form4_xml(self, content: bytes) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ Form 4 XML"""
        if len(content) < 500:  # å¤ªçŸ­çš„å†…å®¹ä¸å¤ªå¯èƒ½æ˜¯æœ‰æ•ˆçš„ Form 4
            return False
        
        content_str = content.decode('utf-8', errors='ignore').lower()
        
        # æ£€æŸ¥å¿…è¦çš„ XML æ ‡ç­¾
        required_tags = ["<ownershipdocument>", "<issuer>"]
        for tag in required_tags:
            if tag not in content_str:
                return False
        
        # æ£€æŸ¥æ˜¯å¦ç¡®å®æ˜¯ Form 4
        form4_indicators = ["form4", "form 4", "ownershipdocument"]
        if not any(indicator in content_str for indicator in form4_indicators):
            return False
            
        return True
