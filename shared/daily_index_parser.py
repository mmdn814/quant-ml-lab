# âœ… æ–‡ä»¶ï¼šshared/daily_index_parser.py
# åŠŸèƒ½ï¼šç”¨äºè§£æ SEC æ¯æ—¥ master.idx æ–‡ä»¶ï¼Œæå– Form 4 filing çš„ accession åˆ—è¡¨ï¼ˆæ”¯æŒå•æ—¥æˆ–å¤šæ—¥ï¼‰

import os
import requests
import logging
from datetime import datetime, timedelta
from typing import List, Tuple

logger = logging.getLogger("insider_ceo")

def get_form4_accessions_from_index(date: datetime, cache_dir: str = ".cache") -> List[Tuple[str, str, str]]:
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€æœ‰ Form 4 æŠ¥å‘Šçš„ CIKã€Accession å’Œå®Œæ•´ filing URLã€‚

    å‚æ•°ï¼š
        date (datetime): æŒ‡å®šæ—¥æœŸï¼ˆä¾‹å¦‚ datetime(2025, 7, 2)ï¼‰
        cache_dir (str): æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œé¿å…é‡å¤ä¸‹è½½ idx æ–‡ä»¶

    è¿”å›ï¼š
        List[(CIK, Accession, Filing URL)]
    """
    quarter = f"Q{((date.month - 1) // 3) + 1}"
    url = f"https://www.sec.gov/Archives/edgar/daily-index/{date.year}/{quarter}/master.{date.strftime('%Y%m%d')}.idx"

    headers = {"User-Agent": "quant-ml-lab/1.0 (mmdn814@gmail.com)"}
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, f"master.{date.strftime('%Y%m%d')}.idx")

    if os.path.exists(cache_path):
        logger.info(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„ master.idx æ–‡ä»¶: {cache_path}")
        with open(cache_path, "r") as f:
            lines = f.readlines()
    else:
        try:
            logger.info(f"ğŸŒ ä¸‹è½½ master.idx æ–‡ä»¶: {url}")
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            lines = res.text.splitlines()
            with open(cache_path, "w") as f:
                f.write(res.text)
        except requests.RequestException as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {url}")
            logger.error(f"åŸå› : {e}")
            return []

    logger.info(f"ğŸ“„ è§£æ master.idx æ–‡ä»¶: {url}ï¼Œå…± {len(lines)} è¡Œ")

    start = False
    results = []
    for line in lines:
        if not start:
            if line.strip().startswith("CIK|Company Name"):
                start = True
            continue

        parts = line.strip().split("|")
        if len(parts) != 5:
            continue
        cik, name, form_type, filed_date, filename = parts
        if form_type.strip() != "4":
            continue

        accession = filename.split("/")[-1].replace(".txt", "")
        url = f"https://www.sec.gov/Archives/{filename}"
        results.append((cik.zfill(10), accession, url))

    logger.info(f"âœ… {date.strftime('%Y-%m-%d')} æå– Form 4 æ¡æ•°: {len(results)}")
    return results

def get_form4_accessions_range(days_back: int, cache_dir: str = ".cache") -> List[Tuple[str, str, str]]:
    """
    è·å–è¿‡å» days_back å¤©å†…çš„æ‰€æœ‰ Form 4 æŠ¥å‘Šï¼ˆå«å»é‡ï¼‰ã€‚

    å‚æ•°ï¼š
        days_back (int): å‘å‰å›æº¯çš„å¤©æ•°ï¼Œä¾‹å¦‚ 3 è¡¨ç¤ºä»Šå¤©ã€æ˜¨å¤©ã€å‰å¤©
        cache_dir (str): idx æ–‡ä»¶ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ .cache

    è¿”å›ï¼š
        List[(CIK, Accession, Filing URL)]
    """
    all_results = []
    seen = set()  # ç”¨äºå»é‡ accession
    for i in range(days_back):
        date = datetime.now() - timedelta(days=i)
        logger.info(f"ğŸ” æ­£åœ¨å¤„ç†æ—¥æœŸ: {date.strftime('%Y-%m-%d')}")
        daily_results = get_form4_accessions_from_index(date, cache_dir=cache_dir)
        logger.info(f"ğŸ“Œ å½“æ—¥ Form 4 æå–æ•°é‡: {len(daily_results)}")
        for cik, accession, url in daily_results:
            if accession not in seen:
                seen.add(accession)
                all_results.append((cik, accession, url))
    logger.info(f"ğŸ“¦ å›æº¯ {days_back} å¤©å…±æå– Form 4 æ¡ç›®: {len(all_results)}")
    return all_results
